{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWRwJuNhdDLX",
        "outputId": "c5cf2167-1fea-490c-bf17-8c22703c05f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [C\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [C\r                                                                               \rHit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connected to r2u.stat.i\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "texlive-fonts-extra is already the newest version (2021.20220204-1).\n",
            "texlive-fonts-recommended is already the newest version (2021.20220204-1).\n",
            "texlive-fonts-recommended set to manually installed.\n",
            "texlive-latex-base is already the newest version (2021.20220204-1).\n",
            "texlive-latex-base set to manually installed.\n",
            "texlive-latex-extra is already the newest version (2021.20220204-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y texlive-latex-base texlive-fonts-recommended texlive-fonts-extra texlive-latex-extra"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cv_template.tex\n",
        "%-------------------------\n",
        "% Resume Template in LaTeX\n",
        "%------------------------\n",
        "\n",
        "\\documentclass[letterpaper,11pt]{article}\n",
        "\n",
        "\\usepackage{latexsym}\n",
        "\\usepackage[empty]{fullpage}\n",
        "\\usepackage{titlesec}\n",
        "\\usepackage{marvosym}\n",
        "\\usepackage{enumitem}\n",
        "\\usepackage[usenames,dvipsnames]{color}\n",
        "\\usepackage{verbatim}\n",
        "\\usepackage[pdftex]{hyperref}\n",
        "\\usepackage{fancyhdr}\n",
        "\n",
        "\\pagestyle{fancy}\n",
        "\\fancyhf{}\n",
        "\\fancyfoot{}\n",
        "\\renewcommand{\\headrulewidth}{0pt}\n",
        "\\renewcommand{\\footrulewidth}{0pt}\n",
        "\n",
        "\\addtolength{\\oddsidemargin}{-0.375in}\n",
        "\\addtolength{\\evensidemargin}{-0.375in}\n",
        "\\addtolength{\\textwidth}{1in}\n",
        "\\addtolength{\\topmargin}{-.5in}\n",
        "\\addtolength{\\textheight}{1.0in}\n",
        "\n",
        "\\urlstyle{same}\n",
        "\\raggedbottom\n",
        "\\raggedright\n",
        "\\setlength{\\tabcolsep}{0in}\n",
        "\n",
        "\\titleformat{\\section}{\n",
        "  \\vspace{-4pt}\\scshape\\raggedright\\large\n",
        "}{}{0em}{}[\\color{black}\\titlerule \\vspace{-5pt}]\n",
        "\n",
        "\\newcommand{\\resumeItem}[2]{\n",
        "  \\item\\small{\n",
        "    \\textbf{#1}{: #2 \\vspace{-2pt}}\n",
        "  }\n",
        "}\n",
        "\n",
        "\\newcommand{\\resumeSubheading}[4]{\n",
        "  \\vspace{-1pt}\\item\n",
        "    \\begin{tabular*}{0.97\\textwidth}{l@{\\extracolsep{\\fill}}r}\n",
        "      \\textbf{#1} & #2 \\\\\n",
        "      \\textit{\\small#3} & \\textit{\\small #4} \\\\\n",
        "    \\end{tabular*}\\vspace{-5pt}\n",
        "}\n",
        "\n",
        "\\newcommand{\\resumeSubItem}[2]{\\resumeItem{#1}{#2}\\vspace{-4pt}}\n",
        "\n",
        "\\renewcommand{\\labelitemii}{$\\circ$}\n",
        "\\newcommand{\\resumeSubHeadingListStart}{\\begin{itemize}[leftmargin=*]}\n",
        "\\newcommand{\\resumeSubHeadingListEnd}{\\end{itemize}}\n",
        "\\newcommand{\\resumeItemListStart}{\\begin{itemize}}\n",
        "\\newcommand{\\resumeItemListEnd}{\\end{itemize}\\vspace{-5pt}}\n",
        "\n",
        "\\begin{document}\n",
        "\n",
        "%----------HEADING-----------------\n",
        "\\begin{tabular*}{\\textwidth}{l@{\\extracolsep{\\fill}}r}\n",
        "  \\textbf{\\href{{website}}{\\Large {name}}} & Email : \\href{mailto:{email}}{ {email} }\\\\\n",
        "  \\href{{website}}{{website}} & Mobile : {mobile} \\\\\n",
        "\\end{tabular*}\n",
        "\n",
        "%-----------EDUCATION-----------------\n",
        "\\section{Education}\n",
        "  \\resumeSubHeadingListStart\n",
        "{education}\n",
        "  \\resumeSubHeadingListEnd\n",
        "\n",
        "%-----------EXPERIENCE-----------------\n",
        "\\section{Experience}\n",
        "  \\resumeSubHeadingListStart\n",
        "{experience}\n",
        "  \\resumeSubHeadingListEnd\n",
        "\n",
        "%-----------PROJECTS-----------------\n",
        "\\section{Projects}\n",
        "  \\resumeSubHeadingListStart\n",
        "{projects}\n",
        "  \\resumeSubHeadingListEnd\n",
        "\n",
        "%--------PROGRAMMING SKILLS------------\n",
        "\\section{Programming Skills}\n",
        " \\resumeSubHeadingListStart\n",
        "   \\item{\n",
        "     \\textbf{Languages}{: {languages}}\n",
        "     \\hfill\n",
        "     \\textbf{Technologies}{: {technologies}}\n",
        "   }\n",
        " \\resumeSubHeadingListEnd\n",
        "\n",
        "\\end{document}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZxVBD3QhMOM",
        "outputId": "b0b53653-de12-4ec4-9432-857561442865"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cv_template.tex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cv_data.csv\n",
        "name,email,mobile,website,education,experience,projects,languages,technologies\n",
        "Amit Sharma,amit@sharma.io,+1-234-567-8901,http://www.amitsharma.io,\"{\\resumeSubheading{Stanford University}{Stanford, CA}{Master of Science in Computer Science; GPA: 3.95}{Sep. 2013 -- Dec. 2014}\\resumeSubheading{Indian Institute of Technology}{Delhi, India}{Bachelor of Technology in Computer Science; GPA: 3.70 (9.25/10.0)}{Aug. 2009 -- July. 2013}}\",\"{\\resumeSubheading{Microsoft}{Redmond, WA}{Software Engineer}{Nov. 2017 -- Present}\\resumeItemListStart\\resumeItem{Azure ML}{Developed and optimized scalable machine learning pipelines for Azure, integrating distributed training systems to handle terabyte-scale datasets, achieving a 25\\% improvement in model deployment efficiency and reducing training time by 20\\%}\\resumeItem{Data Processing}{Designed and implemented high-throughput data ingestion pipelines using Apache Spark, processing millions of records daily, resulting in a 35\\% reduction in processing latency and improved data availability for analytics teams}\\resumeItem{Visualization Tools}{Created and deployed internal BI dashboards with real-time analytics for cross-team collaboration, incorporating interactive features that enhanced decision-making and improved team efficiency by 30\\%}\\resumeItemListEnd\\resumeSubheading{Amazon}{Seattle, WA}{Software Engineer}{Feb. 2015 -- Oct. 2017}\\resumeItemListStart\\resumeItem{Recommendation Engine}{Led the development of a personalized recommendation system for e-commerce platforms, utilizing collaborative filtering and deep learning models, increasing sales conversions by 15\\% and improving user engagement metrics}\\resumeItem{Data Pipeline}{Designed and deployed ETL pipelines for real-time inventory updates using AWS Glue, ensuring seamless data flow across distributed systems and reducing data sync errors by 40\\%}\\resumeItem{Automation Tools}{Developed and maintained automation scripts for testing and deployment processes, leveraging Python and Jenkins, which reduced manual testing time by 60\\% and streamlined CI/CD workflows}\\resumeItem{User Analytics}{Built a comprehensive user behavior tracking system integrated with Redshift, enabling real-time insights into customer interactions and supporting data-driven product improvements}\\resumeItemListEnd\\resumeSubheading{Open Source Contributions}{Remote}{Contributor}{2014 -- 2017}\\resumeItemListStart\\resumeItem{ML Libraries}{Contributed advanced features to scikit-learn, including new model evaluation metrics and optimization algorithms, adopted by thousands of developers for machine learning workflows}\\resumeItem{Web Tools}{Developed and enhanced Plotly components for interactive data visualizations, enabling dynamic charting capabilities used in over 500 open-source projects}\\resumeItem{Tech Writing}{Authored over 40 detailed blog posts on cloud computing, machine learning pipelines, and scalable architectures, gaining a wide readership in the data science community}\\resumeItemListEnd\\resumeSubheading{Quantiphi}{Boston, MA}{Data Engineer}{Summer 2013}\\resumeItemListStart\\resumeItem{AI Models}{Developed and fine-tuned NLP models for text classification in client-facing projects, achieving a 90\\% accuracy rate and enabling real-time sentiment analysis for business applications}\\resumeItem{Data Platform}{Built a scalable data ingestion system for real-time analytics, integrating multiple data sources and reducing data processing bottlenecks by 25\\%}\\resumeItemListEnd}\",\"{\\resumeSubItem{AI Chatbot}{Built an NLP-based chatbot for customer support, deployed on AWS}\\resumeSubItem{Data Dashboard}{Created interactive dashboards for sales analytics using D3.js}\\resumeSubItem{Prediction System}{Designed a demand forecasting model for retail using public datasets}\\resumeSubItem{ETL Framework}{Developed a scalable ETL framework for multi-source data integration}}\",\"Python, Java, JavaScript, SQL, Go\",\"AWS, Kubernetes, React, Spark, Snowflake\"\n",
        "Priya Patel,priya@patelworks.com,+1-345-678-9012,http://www.priyapatelworks.com,\"{\\resumeSubheading{MIT}{Cambridge, MA}{Master of Science in Data Science; GPA: 4.00}{Sep. 2014 -- Dec. 2015}\\resumeSubheading{Anna University}{Chennai, India}{Bachelor of Engineering in Computer Science; GPA: 3.80 (9.50/10.0)}{Aug. 2010 -- July. 2014}}\",\"{\\resumeSubheading{Facebook}{Menlo Park, CA}{Data Engineer}{Jan. 2018 -- Present}\\resumeItemListStart\\resumeItem{Graph Analytics}{Developed and optimized graph-based algorithms for social network analysis, leveraging large-scale data to improve ad targeting precision by 20\\% and enhance user engagement metrics}\\resumeItem{Data Pipelines}{Designed and implemented streaming data pipelines using Kafka, handling millions of events per second, reducing latency by 40\\% and enabling real-time analytics for ad campaigns}\\resumeItem{Dashboards}{Created and maintained real-time monitoring dashboards for ad performance, integrating with internal tools to provide actionable insights and improve campaign efficiency by 25\\%}\\resumeItemListEnd\\resumeSubheading{Uber}{San Francisco, CA}{Software Engineer}{Mar. 2016 -- Dec. 2017}\\resumeItemListStart\\resumeItem{Geo Analytics}{Developed geospatial analysis tools for ride optimization, integrating real-time traffic data to improve route efficiency and reduce trip times by 15\\%}\\resumeItem{ETL Systems}{Built and maintained ETL pipelines for driver data using Airflow and BigQuery, ensuring seamless data integration across platforms and reducing data errors by 30\\%}\\resumeItem{API Development}{Designed and deployed REST APIs for real-time trip data access, enabling third-party integrations and improving data accessibility for analytics teams}\\resumeItem{Fraud Detection}{Developed and implemented anomaly detection models for payment systems, leveraging machine learning to reduce fraudulent transactions by 20\\%}\\resumeItemListEnd\\resumeSubheading{Open Source Contributions}{Remote}{Contributor}{2015 -- 2017}\\resumeItemListStart\\resumeItem{NLP Libraries}{Contributed advanced text processing features to spaCy, including tokenization and entity recognition, adopted by numerous NLP applications}\\resumeItem{Visualization}{Developed Vega.js components for interactive charts, enabling dynamic visualizations for time-series data in over 300 projects}\\resumeItem{Blogging}{Authored over 45 detailed blog posts on data engineering best practices, focusing on scalable pipelines and cloud architectures}\\resumeItemListEnd\\resumeSubheading{ZS Associates}{Evanston, IL}{Data Analyst}{Summer 2014}\\resumeItemListStart\\resumeItem{Predictive Models}{Built and fine-tuned churn prediction models for retail clients, achieving 85\\% accuracy and enabling targeted retention strategies}\\resumeItem{Reporting Tools}{Developed and automated reporting systems for sales data, integrating with Tableau to provide real-time insights for client decision-making}\\resumeItemListEnd}\",\"{\\resumeSubItem{Sentiment Analysis}{Built a sentiment analysis tool for social media data}\\resumeSubItem{Interactive Maps}{Created geospatial visualizations for urban planning}\\resumeSubItem{Forecasting Model}{Designed a time-series forecasting system for logistics}\\resumeSubItem{Data Integration}{Developed an ETL pipeline for cross-platform data syncing}}\",\"Python, Scala, SQL, R, JavaScript\",\"AWS, Kafka, Tableau, BigQuery, Hadoop\"\n",
        "Rahul Gupta,rahul@guptatech.com,+1-456-789-0123,http://www.rahulguptatech.com,\"{\\resumeSubheading{Carnegie Mellon University}{Pittsburgh, PA}{Master of Science in Artificial Intelligence; GPA: 3.90}{Aug. 2013 -- Dec. 2014}\\resumeSubheading{NIT Trichy}{Trichy, India}{Bachelor of Technology in Computer Science; GPA: 3.75 (9.38/10.0)}{Aug. 2009 -- July. 2013}}\",\"{\\resumeSubheading{Apple}{Cupertino, CA}{Machine Learning Engineer}{Feb. 2018 -- Present}\\resumeItemListStart\\resumeItem{CoreML}{Optimized on-device machine learning models for iOS applications, leveraging CoreML to improve inference speed by 30\\% and reduce battery consumption for end users}\\resumeItem{Data Pipelines}{Designed and implemented data processing pipelines for Siri analytics using Spark, handling large-scale datasets to enable real-time insights and improve feature performance}\\resumeItem{Tools}{Developed and maintained internal tools for model evaluation and monitoring, integrating with dashboards to streamline debugging and enhance model accuracy by 25\\%}\\resumeItemListEnd\\resumeSubheading{LinkedIn}{Sunnyvale, CA}{Software Engineer}{Jan. 2015 -- Jan. 2018}\\resumeItemListStart\\resumeItem{Search Engine}{Improved search ranking algorithms by integrating user behavior data, increasing click-through rates by 10\\% and enhancing search relevance across platforms}\\resumeItem{Data Infrastructure}{Built and scaled ETL pipelines for user data using Hive, optimizing data flows to support analytics and reduce processing time by 30\\%}\\resumeItem{APIs}{Designed and deployed REST APIs for profile analytics, enabling seamless data access for internal tools and third-party integrations}\\resumeItem{Analytics Platform}{Created a robust platform for A/B testing of new features, streamlining experimentation and improving feature deployment efficiency by 20\\%}\\resumeItemListEnd\\resumeSubheading{Open Source Contributions}{Remote}{Contributor}{2014 -- 2017}\\resumeItemListStart\\resumeItem{ML Frameworks}{Contributed to PyTorch for distributed training, adding features for large-scale model optimization used in production environments}\\resumeItem{Visualization}{Developed Plotly components for 3D data visualization, enabling interactive charting for complex datasets in over 400 projects}\\resumeItem{Blogging}{Authored over 50 technical articles on machine learning, data pipelines, and scalable systems, widely read in the tech community}\\resumeItemListEnd\\resumeSubheading{Mu Sigma}{Bangalore, India}{Data Scientist}{Summer 2013}\\resumeItemListStart\\resumeItem{Risk Models}{Built and validated risk assessment models for finance clients, achieving high accuracy and enabling data-driven investment strategies}\\resumeItem{Dashboards}{Developed and deployed Tableau dashboards for client reporting, automating data visualization and improving reporting efficiency}\\resumeItemListEnd}\",\"{\\resumeSubItem{Image Recognition}{Built a CNN-based image classification system}\\resumeSubItem{Network Analysis}{Created tools for visualizing network graphs}\\resumeSubItem{Forecasting Tool}{Designed a sales prediction model for e-commerce}\\resumeSubItem{ETL System}{Developed a pipeline for real-time data integration}}\",\"Python, Java, C++, SQL, JavaScript\",\"AWS, TensorFlow, Hive, Tableau, Kubernetes\"\n",
        "Neha Singh,neha@singhcode.com,+1-567-890-1234,http://www.nehasinghcode.com,\"{\\resumeSubheading{UC Berkeley}{Berkeley, CA}{Master of Science in Computer Science; GPA: 3.85}{Sep. 2014 -- Dec. 2015}\\resumeSubheading{VIT University}{Vellore, India}{Bachelor of Technology in Information Technology; GPA: 3.70 (9.25/10.0)}{Aug. 2010 -- July. 2014}}\",\"{\\resumeSubheading{Google}{Seattle, WA}{Software Engineer}{Mar. 2018 -- Present}\\resumeItemListStart\\resumeItem{Cloud AI}{Developed and deployed AI services for Google Cloud, optimizing API response times by 25\\% and enhancing scalability for enterprise clients}\\resumeItem{Data Pipelines}{Built and maintained streaming data pipelines using Pub/Sub and Dataflow, processing millions of events daily to support real-time analytics and reduce latency by 35\\%}\\resumeItem{Dashboards}{Created and enhanced monitoring dashboards for cloud usage analytics, integrating with internal tools to provide actionable insights and improve resource allocation}\\resumeItemListEnd\\resumeSubheading{Salesforce}{San Francisco, CA}{Software Engineer}{Feb. 2016 -- Feb. 2018}\\resumeItemListStart\\resumeItem{CRM Analytics}{Developed advanced analytics tools for customer insights, leveraging machine learning to improve customer segmentation and increase retention by 15\\%}\\resumeItem{ETL Pipelines}{Built and optimized data pipelines for CRM data using Apache NiFi, ensuring seamless data integration and reducing processing errors by 30\\%}\\resumeItem{APIs}{Designed and implemented REST APIs for real-time data access, enabling third-party integrations and improving data accessibility for analytics teams}\\resumeItem{Automation}{Automated data validation and processing workflows, leveraging Python and Jenkins to save 25 hours/week and streamline operations}\\resumeItemListEnd\\resumeSubheading{Open Source Contributions}{Remote}{Contributor}{2015 -- 2018}\\resumeItemListStart\\resumeItem{Data Tools}{Contributed to Apache Airflow for workflow management, adding features for scheduling and monitoring used in production pipelines}\\resumeItem{Visualization}{Developed D3.js components for time-series visualization, enabling interactive charting for over 300 open-source projects}\\resumeItem{Blogging}{Authored over 40 technical blog posts on cloud computing, data engineering, and scalable architectures, widely read in the tech community}\\resumeItemListEnd\\resumeSubheading{Fractal Analytics}{New York, NY}{Data Engineer}{Summer 2014}\\resumeItemListStart\\resumeItem{Predictive Models}{Built and fine-tuned predictive models for customer segmentation, achieving high accuracy and enabling targeted marketing strategies}\\resumeItem{Reporting}{Developed and automated reporting tools for client analytics, integrating with Tableau to provide real-time insights}\\resumeItemListEnd}\",\"{\\resumeSubItem{Chatbot}{Built an AI chatbot for e-commerce support}\\resumeSubItem{Visualization Tool}{Created interactive dashboards for sales data}\\resumeSubItem{Prediction Model}{Designed a churn prediction system for SaaS}\\resumeSubItem{ETL Framework}{Developed a pipeline for multi-source data integration}}\",\"Python, Scala, SQL, JavaScript, R\",\"AWS, Airflow, Tableau, Snowflake, Kafka\"\n",
        "Vikram Rao,vikram@raotech.io,+1-678-901-2345,http://www.vikramrao.io,\"{\\resumeSubheading{University of Washington}{Seattle, WA}{Master of Science in Data Science; GPA: 3.90}{Sep. 2013 -- Dec. 2014}\\resumeSubheading{IIT Madras}{Chennai, India}{Bachelor of Technology in Computer Science; GPA: 3.80 (9.50/10.0)}{Aug. 2009 -- July. 2013}}\",\"{\\resumeSubheading{Amazon}{Seattle, WA}{Data Engineer}{Apr. 2018 -- Present}\\resumeItemListStart\\resumeItem{AWS Redshift}{Optimized data warehousing solutions with AWS Redshift, implementing partitioning and indexing strategies to reduce query times by 30\\% and improve analytics performance}\\resumeItem{Data Pipelines}{Designed and deployed ETL pipelines for real-time analytics using Kinesis, processing large-scale datasets to support inventory tracking and reduce data latency by 35\\%}\\resumeItem{Dashboards}{Created and maintained BI dashboards for inventory tracking, integrating with internal tools to provide real-time insights and improve operational efficiency by 25\\%}\\resumeItemListEnd\\resumeSubheading{Twitter}{San Francisco, CA}{Software Engineer}{Jan. 2015 -- Mar. 2018}\\resumeItemListStart\\resumeItem{Ad Analytics}{Developed advanced analytics tools for ad performance, leveraging machine learning to optimize targeting and increase ad revenue by 15\\%}\\resumeItem{Streaming Data}{Built and optimized streaming pipelines for tweet data using Storm, handling millions of events daily and reducing processing latency by 30\\%}\\resumeItem{APIs}{Designed and deployed REST APIs for real-time analytics, enabling seamless data access for internal tools and third-party integrations}\\resumeItem{Fraud Detection}{Developed and implemented anomaly detection models for bot detection, leveraging machine learning to reduce fraudulent accounts by 20\\%}\\resumeItemListEnd\\resumeSubheading{Open Source Contributions}{Remote}{Contributor}{2014 -- 2017}\\resumeItemListStart\\resumeItem{ML Libraries}{Contributed to XGBoost for gradient boosting, adding features for large-scale model training used in production environments}\\resumeItem{Visualization}{Developed Chart.js components for data visualization, enabling interactive charting for over 400 open-source projects}\\resumeItem{Blogging}{Authored over 50 technical blog posts on data pipelines, machine learning, and scalable systems, widely read in the tech community}\\resumeItemListEnd\\resumeSubheading{Capgemini}{Chicago, IL}{Data Analyst}{Summer 2013}\\resumeItemListStart\\resumeItem{Risk Models}{Built and validated risk assessment models for finance clients, achieving high accuracy and enabling data-driven investment strategies}\\resumeItem{Reporting}{Developed and deployed Tableau dashboards for client reporting, automating data visualization and improving reporting efficiency}\\resumeItemListEnd}\",\"{\\resumeSubItem{Fraud Detection}{Built an anomaly detection system for payments}\\resumeSubItem{Visualization}{Created tools for real-time data visualization}\\resumeSubItem{Forecasting}{Designed a demand forecasting model for retail}\\resumeSubItem{ETL Pipeline}{Developed a scalable data integration framework}}\",\"Python, Java, SQL, JavaScript, Scala\",\"AWS, Kinesis, Tableau, Hadoop, Spark\"\n",
        "Ananya Desai,ananya@desaitech.com,+1-789-012-3456,http://www.ananyadesai.com,\"{\\resumeSubheading{Columbia University}{New York, NY}{Master of Science in Computer Science; GPA: 3.95}{Sep. 2014 -- Dec. 2015}\\resumeSubheading{BITS Pilani}{Pilani, India}{Bachelor of Engineering in Computer Science; GPA: 3.75 (9.38/10.0)}{Aug. 2010 -- July. 2014}}\",\"{\\resumeSubheading{Netflix}{Los Gatos, CA}{Software Engineer}{May. 2018 -- Present}\\resumeItemListStart\\resumeItem{Recommendation Systems}{Improved content recommendation algorithms using deep learning, boosting viewer engagement by 15\\% and enhancing content discovery across platforms}\\resumeItem{Data Pipelines}{Built and optimized streaming pipelines for viewer data using Kafka, processing millions of events daily to support real-time analytics and reduce latency by 30\\%}\\resumeItem{Dashboards}{Created and maintained analytics dashboards for content performance, integrating with internal tools to provide actionable insights and improve content strategy}\\resumeItemListEnd\\resumeSubheading{Adobe}{San Jose, CA}{Software Engineer}{Feb. 2016 -- Apr. 2018}\\resumeItemListStart\\resumeItem{Analytics Tools}{Developed advanced analytics tools for marketing campaigns, leveraging machine learning to improve customer segmentation and increase campaign ROI by 15\\%}\\resumeItem{ETL Pipelines}{Built and optimized data pipelines for customer data using Airflow, ensuring seamless data integration and reducing processing errors by 30\\%}\\resumeItem{APIs}{Designed and implemented REST APIs for real-time data access, enabling third-party integrations and improving data accessibility for analytics teams}\\resumeItem{Automation}{Automated data processing and validation workflows, leveraging Python and Jenkins to save 25 hours/week and streamline operations}\\resumeItemListEnd\\resumeSubheading{Open Source Contributions}{Remote}{Contributor}{2015 -- 2018}\\resumeItemListStart\\resumeItem{ML Libraries}{Contributed to LightGBM for faster model training, adding features for large-scale datasets used in production environments}\\resumeItem{Visualization}{Developed D3.js components for interactive charts, enabling dynamic visualizations for over 300 open-source projects}\\resumeItem{Blogging}{Authored over 45 technical blog posts on data science, machine learning, and scalable architectures, widely read in the tech community}\\resumeItemListEnd\\resumeSubheading{Tiger Analytics}{Chicago, IL}{Data Scientist}{Summer 2014}\\resumeItemListStart\\resumeItem{Predictive Models}{Built and fine-tuned predictive models for customer retention, achieving high accuracy and enabling targeted marketing strategies}\\resumeItem{Reporting}{Developed and automated reporting tools for client analytics, integrating with Tableau to provide real-time insights}\\resumeItemListEnd}\",\"{\\resumeSubItem{Recommendation Engine}{Built a content recommendation system}\\resumeSubItem{Visualization Tool}{Created dashboards for viewer analytics}\\resumeSubItem{Forecasting Model}{Designed a viewership prediction system}\\resumeSubItem{ETL Framework}{Developed a pipeline for multi-source data integration}}\",\"Python, Scala, SQL, JavaScript, R\",\"AWS, Kafka, Tableau, Snowflake, Hadoop\"\n",
        "Kiran Malhotra,kiran@malhotratech.com,+1-890-123-4567,http://www.kiranmalhotra.com,\"{\\resumeSubheading{University of Texas}{Austin, TX}{Master of Science in Data Science; GPA: 3.90}{Sep. 2013 -- Dec. 2014}\\resumeSubheading{IIT Bombay}{Mumbai, India}{Bachelor of Technology in Computer Science; GPA: 3.80 (9.50/10.0)}{Aug. 2009 -- July. 2013}}\",\"{\\resumeSubheading{IBM}{Austin, TX}{Data Engineer}{Jun. 2018 -- Present}\\resumeItemListStart\\resumeItem{Watson AI}{Developed and deployed AI services for Watson, integrating advanced NLP models to improve accuracy by 20\\% and enhance user interaction for enterprise clients}\\resumeItem{Data Pipelines}{Built and optimized ETL pipelines for analytics using Apache NiFi, processing large-scale datasets to support real-time insights and reduce latency by 35\\%}\\resumeItem{Dashboards}{Created and maintained monitoring dashboards for AI performance, integrating with internal tools to provide actionable insights and improve operational efficiency}\\resumeItemListEnd\\resumeSubheading{PayPal}{San Jose, CA}{Software Engineer}{Jan. 2015 -- May. 2018}\\resumeItemListStart\\resumeItem{Fraud Detection}{Developed and implemented anomaly detection models for transaction monitoring, leveraging machine learning to reduce fraudulent transactions by 20\\% and improve security}\\resumeItem{ETL Pipelines}{Built and optimized data pipelines for payment data using Kafka, ensuring seamless data integration and reducing processing errors by 30\\%}\\resumeItem{APIs}{Designed and deployed REST APIs for real-time analytics, enabling third-party integrations and improving data accessibility for analytics teams}\\resumeItem{Analytics Platform}{Created a robust platform for transaction analytics, streamlining data processing and enabling real-time insights for business decisions}\\resumeItemListEnd\\resumeSubheading{Open Source Contributions}{Remote}{Contributor}{2014 -- 2017}\\resumeItemListStart\\resumeItem{ML Libraries}{Contributed to TensorFlow for distributed training, adding features for large-scale model optimization used in production environments}\\resumeItem{Visualization}{Developed Plotly components for data visualization, enabling interactive charting for over 400 open-source projects}\\resumeItem{Blogging}{Authored over 50 technical blog posts on AI, data engineering, and scalable systems, widely read in the tech community}\\resumeItemListEnd\\resumeSubheading{Mu Sigma}{Bangalore, India}{Data Analyst}{Summer 2013}\\resumeItemListStart\\resumeItem{Risk Models}{Built and validated risk assessment models for finance clients, achieving high accuracy and enabling data-driven investment strategies}\\resumeItem{Reporting}{Developed and deployed Tableau dashboards for client reporting, automating data visualization and improving reporting efficiency}\\resumeItemListEnd}\",\"{\\resumeSubItem{Anomaly Detection}{Built a system for fraud detection in payments}\\resumeSubItem{Visualization Tool}{Created dashboards for transaction analytics}\\resumeSubItem{Prediction Model}{Designed a churn prediction system for fintech}\\resumeSubItem{ETL Pipeline}{Developed a scalable data integration framework}}\",\"Python, Java, SQL, JavaScript, Scala\",\"AWS, Kafka, Tableau, Hadoop, Spark\"\n",
        "Rohan Mehra,rohan@mehratech.io,+1-901-234-5678,http://www.rohanmehra.io,\"{\\resumeSubheading{University of Michigan}{Ann Arbor, MI}{Master of Science in Computer Science; GPA: 3.95}{Sep. 2014 -- Dec. 2015}\\resumeSubheading{NIT Surathkal}{Surathkal, India}{Bachelor of Technology in Computer Science; GPA: 3.75 (9.38/10.0)}{Aug. 2010 -- July. 2014}}\",\"{\\resumeSubheading{Spotify}{New York, NY}{Software Engineer}{Jul. 2018 -- Present}\\resumeItemListStart\\resumeItem{Recommendation Systems}{Improved music recommendation algorithms using deep learning, boosting listener engagement by 18\\% and enhancing playlist personalization across platforms}\\resumeItem{Data Pipelines}{Built and optimized streaming pipelines for listener data using Kafka, processing millions of events daily to support real-time analytics and reduce latency by 30\\%}\\resumeItem{Dashboards}{Created and maintained analytics dashboards for artist performance, integrating with internal tools to provide actionable insights and improve content strategy}\\resumeItemListEnd\\resumeSubheading{Square}{San Francisco, CA}{Software Engineer}{Feb. 2016 -- Jun. 2018}\\resumeItemListStart\\resumeItem{Payment Analytics}{Developed advanced analytics tools for transaction monitoring, leveraging machine learning to optimize payment processing and increase revenue by 15\\%}\\resumeItem{ETL Pipelines}{Built and optimized data pipelines for payment data using Airflow, ensuring seamless data integration and reducing processing errors by 30\\%}\\resumeItem{APIs}{Designed and deployed REST APIs for real-time analytics, enabling third-party integrations and improving data accessibility for analytics teams}\\resumeItem{Fraud Detection}{Developed and implemented anomaly detection models for payment systems, leveraging machine learning to reduce fraudulent transactions by 20\\%}\\resumeItemListEnd\\resumeSubheading{Open Source Contributions}{Remote}{Contributor}{2015 -- 2018}\\resumeItemListStart\\resumeItem{ML Libraries}{Contributed to scikit-learn for model evaluation, adding features for large-scale datasets used in production environments}\\resumeItem{Visualization}{Developed D3.js components for interactive charts, enabling dynamic visualizations for over 300 open-source projects}\\resumeItem{Blogging}{Authored over 40 technical blog posts on data science, machine learning, and scalable architectures, widely read in the tech community}\\resumeItemListEnd\\resumeSubheading{Fractal Analytics}{New York, NY}{Data Scientist}{Summer 2014}\\resumeItemListStart\\resumeItem{Predictive Models}{Built and fine-tuned predictive models for customer retention, achieving high accuracy and enabling targeted marketing strategies}\\resumeItem{Reporting}{Developed and automated reporting tools for client analytics, integrating with Tableau to provide real-time insights}\\resumeItemListEnd}\",\"{\\resumeSubItem{Recommendation Engine}{Built a music recommendation system}\\resumeSubItem{Visualization Tool}{Created dashboards for listener analytics}\\resumeSubItem{Forecasting Model}{Designed a listener retention prediction system}\\resumeSubItem{ETL Framework}{Developed a pipeline for multi-source data integration}}\",\"Python, Scala, SQL, JavaScript, R\",\"AWS, Kafka, Tableau, Snowflake, Hadoop\"\n",
        "Shalini Nair,shalini@nairtech.com,+1-012-345-6789,http://www.shalininair.com,\"{\\resumeSubheading{University of Southern California}{Los Angeles, CA}{Master of Science in Data Science; GPA: 3.90}{Sep. 2013 -- Dec. 2014}\\resumeSubheading{Anna University}{Chennai, India}{Bachelor of Engineering in Computer Science; GPA: 3.80 (9.50/10.0)}{Aug. 2009 -- July. 2013}}\",\"{\\resumeSubheading{Pinterest}{San Francisco, CA}{Data Engineer}{Aug. 2018 -- Present}\\resumeItemListStart\\resumeItem{Recommendation Systems}{Improved pin recommendation algorithms using machine learning, increasing user engagement by 20\\% and enhancing content discovery across platforms}\\resumeItem{Data Pipelines}{Built and optimized streaming pipelines for user data using Kafka, processing millions of events daily to support real-time analytics and reduce latency by 30\\%}\\resumeItem{Dashboards}{Created and maintained analytics dashboards for ad performance, integrating with internal tools to provide actionable insights and improve campaign efficiency}\\resumeItemListEnd\\resumeSubheading{Dropbox}{San Francisco, CA}{Software Engineer}{Jan. 2015 -- Jul. 2018}\\resumeItemListStart\\resumeItem{Analytics Tools}{Developed advanced analytics tools for user behavior tracking, leveraging machine learning to improve file sharing insights and increase user retention by 15\\%}\\resumeItem{ETL Pipelines}{Built and optimized data pipelines for file metadata using Airflow, ensuring seamless data integration and reducing processing errors by 30\\%}\\resumeItem{APIs}{Designed and deployed REST APIs for real-time data access, enabling third-party integrations and improving data accessibility for analytics teams}\\resumeItem{Automation}{Automated data validation and processing workflows, leveraging Python and Jenkins to save 25 hours/week and streamline operations}\\resumeItemListEnd\\resumeSubheading{Open Source Contributions}{Remote}{Contributor}{2014 -- 2017}\\resumeItemListStart\\resumeItem{ML Libraries}{Contributed to LightGBM for faster model training, adding features for large-scale datasets used in production environments}\\resumeItem{Visualization}{Developed Plotly components for data visualization, enabling interactive charting for over 400 open-source projects}\\resumeItem{Blogging}{Authored over 50 technical blog posts on data pipelines, machine learning, and scalable systems, widely read in the tech community}\\resumeItemListEnd\\resumeSubheading{Capgemini}{ Federica}{Chicago, IL}{Data Analyst}{Summer 2013}\\resumeItemListStart\\resumeItem{Risk Models}{Built and validated risk assessment models for finance clients, achieving high accuracy and enabling data-driven investment strategies}\\resumeItem{Reporting}{Developed and deployed Tableau dashboards for client reporting, automating data visualization and improving reporting efficiency}\\resumeItemListEnd}\",\"{\\resumeSubItem{Recommendation Engine}{Built a pin recommendation system}\\resumeSubItem{Visualization Tool}{Created dashboards for user analytics}\\resumeSubItem{Forecasting Model}{Designed a user engagement prediction system}\\resumeSubItem{ETL Pipeline}{Developed a scalable data integration framework}}\",\"Python, Java, SQL, JavaScript, Scala\",\"AWS, Kafka, Tableau, Hadoop, Spark\"\n",
        "Aditya Joshi,aditya@joshtech.io,+1-123-456-7890,http://www.adityajoshi.io,\"{\\resumeSubheading{University of California, San Diego}{San Diego, CA}{Master of Science in Computer Science; GPA: 3.95}{Sep. 2014 -- Dec. 2015}\\resumeSubheading{IIT Delhi}{Delhi, India}{Bachelor of Technology in Computer Science; GPA: 3.75 (9.38/10.0)}{Aug. 2010 -- July. 2014}}\",\"{\\resumeSubheading{Airbnb}{San Francisco, CA}{Software Engineer}{Sep. 2018 -- Present}\\resumeItemListStart\\resumeItem{Search Algorithms}{Improved search ranking algorithms by integrating user behavior data, increasing booking conversions by 15\\% and enhancing search relevance across platforms}\\resumeItem{Data Pipelines}{Built and optimized streaming pipelines for booking data using Kafka, processing millions of events daily to support real-time analytics and reduce latency by 30\\%}\\resumeItem{Dashboards}{Created and maintained analytics dashboards for host performance, integrating with internal tools to provide actionable insights and improve operational efficiency}\\resumeItemListEnd\\resumeSubheading{Etsy}{New York, NY}{Software Engineer}{Feb. 2016 -- Aug. 2018}\\resumeItemListStart\\resumeItem{Recommendation Engine}{Developed advanced recommendation tools for product suggestions, leveraging machine learning to improve sales conversions and increase revenue by 15\\%}\\resumeItem{ETL Pipelines}{Built and optimized data pipelines for sales data using Airflow, ensuring seamless data integration and reducing processing errors by 30\\%}\\resumeItem{APIs}{Designed and deployed REST APIs for real-time analytics, enabling third-party integrations and improving data accessibility for analytics teams}\\resumeItem{Analytics Platform}{Created a robust platform for seller analytics, streamlining data processing and enabling real-time insights for business decisions}\\resumeItemListEnd\\resumeSubheading{Open Source Contributions}{Remote}{Contributor}{2015 -- 2018}\\resumeItemListStart\\resumeItem{ML Libraries}{Contributed to scikit-learn for model evaluation, adding features for large-scale datasets used in production environments}\\resumeItem{Visualization}{Developed D3.js components for interactive charts, enabling dynamic visualizations for over 300 open-source projects}\\resumeItem{Blogging}{Authored over 40 technical blog posts on data science, machine learning, and scalable architectures, widely read in the tech community}\\resumeItemListEnd\\resumeSubheading{Tiger Analytics}{Chicago, IL}{Data Scientist}{Summer 2014}\\resumeItemListStart\\resumeItem{Predictive Models}{Built and fine-tuned predictive models for customer retention, achieving high accuracy and enabling targeted marketing strategies}\\resumeItem{Reporting}{Developed and automated reporting tools for client analytics, integrating with Tableau to provide real-time insights}\\resumeItemListEnd}\",\"{\\resumeSubItem{Recommendation Engine}{Built a product recommendation system}\\resumeSubItem{Visualization Tool}{Created dashboards for sales analytics}\\resumeSubItem{Forecasting Model}{Designed a booking prediction system}\\resumeSubItem{ETL Framework}{Developed a pipeline for multi-source data integration}}\",\"Python, Scala, SQL, JavaScript, R\",\"AWS, Kafka, Tableau, Snowflake, Hadoop\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hrs3HNqhUVf",
        "outputId": "bf1cd075-e2b4-4f2a-b59c-cef5ea7aa010"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cv_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "def load_template(template_file):\n",
        "    with open(template_file, 'r') as f:\n",
        "        return f.read()\n",
        "\n",
        "def latex_to_pdf(latex_code, output_filename):\n",
        "    tex_file = f\"{output_filename}.tex\"\n",
        "    with open(tex_file, \"w\") as f:\n",
        "        f.write(latex_code)\n",
        "    subprocess.run([\"pdflatex\", tex_file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    for ext in [\"aux\", \"log\", \"tex\"]:\n",
        "        if os.path.exists(f\"{output_filename}.{ext}\"):\n",
        "            os.remove(f\"{output_filename}.{ext}\")\n",
        "\n",
        "def generate_cvs(csv_file, template_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    template = load_template(template_file)\n",
        "    for _, row in df.iterrows():\n",
        "        latex_code = template\n",
        "        for col in df.columns:\n",
        "            latex_code = latex_code.replace(f\"{{{col}}}\", str(row[col]))\n",
        "        output_filename = f\"CV_{row['name'].replace(' ', '_')}\"\n",
        "        latex_to_pdf(latex_code, output_filename)\n",
        "\n",
        "generate_cvs(\"cv_data.csv\", \"cv_template.tex\")\n"
      ],
      "metadata": {
        "id": "qYLHj8aRhWk5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "for file in os.listdir():\n",
        "    if file.endswith(\".pdf\"):\n",
        "        files.download(file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zLFN9QS1haSg",
        "outputId": "4a6bd49c-ae0e-47ef-f6de-372ad1198541"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3d1d2788-107b-41a2-815b-9840457f40b7\", \"CV_Rahul_Gupta.pdf\", 95297)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9eb50005-7768-42cf-b6b3-0b855ea526e9\", \"CV_Kiran_Malhotra.pdf\", 94032)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_083d7c4e-3aa3-4689-96f4-c87de7ae5ff4\", \"CV_Shalini_Nair.pdf\", 93680)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_586dbed0-fdc7-457e-9717-287f26d70b8a\", \"CV_Vikram_Rao.pdf\", 94643)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_57e9c9e5-35eb-4b83-99c6-7ee212df78bb\", \"CV_Aditya_Joshi.pdf\", 94500)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0b46ea1d-3dbd-443e-ae15-2f22022a65e7\", \"CV_Rohan_Mehra.pdf\", 94314)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9ae58307-c7a1-4b89-9f4e-ced1131a06dd\", \"CV_Neha_Singh.pdf\", 94426)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fcfb5e4e-167e-4878-80db-07541b790f47\", \"CV_Amit_Sharma.pdf\", 94975)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_63e51007-102d-496f-8671-7f7778c7753d\", \"CV_Ananya_Desai.pdf\", 94345)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0200f710-c946-4524-bbc2-ebaff0ecbd46\", \"CV_Priya_Patel.pdf\", 93926)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_files_with_extensions(extensions):\n",
        "    for root, dirs, files in os.walk('.'):\n",
        "        for file in files:\n",
        "            if file.endswith(extensions):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    os.remove(file_path)\n",
        "                    print(f\"Deleted: {file_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "# Delete .out and .pdf files\n",
        "delete_files_with_extensions(('.out', '.pdf'))\n",
        "\n",
        "print(\"Deletion complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkrGshGBz3pr",
        "outputId": "29f9aca0-f1c5-4714-9211-4826c1c1066f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: ./CV_Ananya_Desai.out\n",
            "Deleted: ./CV_Rahul_Gupta.pdf\n",
            "Deleted: ./CV_Shalini_Nair.out\n",
            "Deleted: ./CV_Rahul_Gupta.out\n",
            "Deleted: ./CV_Kiran_Malhotra.pdf\n",
            "Deleted: ./CV_Shalini_Nair.pdf\n",
            "Deleted: ./CV_Vikram_Rao.pdf\n",
            "Deleted: ./CV_Aditya_Joshi.pdf\n",
            "Deleted: ./CV_Rohan_Mehra.pdf\n",
            "Deleted: ./CV_Amit_Sharma.out\n",
            "Deleted: ./CV_Neha_Singh.pdf\n",
            "Deleted: ./CV_Amit_Sharma.pdf\n",
            "Deleted: ./CV_Rohan_Mehra.out\n",
            "Deleted: ./CV_Neha_Singh.out\n",
            "Deleted: ./CV_Ananya_Desai.pdf\n",
            "Deleted: ./CV_Priya_Patel.out\n",
            "Deleted: ./CV_Kiran_Malhotra.out\n",
            "Deleted: ./CV_Priya_Patel.pdf\n",
            "Deleted: ./CV_Vikram_Rao.out\n",
            "Deleted: ./CV_Aditya_Joshi.out\n",
            "Deletion complete!\n"
          ]
        }
      ]
    }
  ]
}